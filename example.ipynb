{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example use case of NewtonCG optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from newton_cg import NewtonCG\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "### We use MNIST\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(\"~/Downloads/\", transform=transform)\n",
    "test_set = datasets.MNIST(\"~/Downloads/\", transform=transform, train=False)\n",
    "train_loader = DataLoader(train_set, batch_size=len(train_set))\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set))\n",
    "\n",
    "### NewtonCG assumes optimization steps are performed over the full dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_inputs, train_targets = iter(train_loader).next()\n",
    "train_inputs = train_inputs.reshape(-1, 784).to(device)\n",
    "train_targets = train_targets.to(device)\n",
    "test_inputs, test_targets = iter(test_loader).next()\n",
    "test_inputs = test_inputs.reshape(-1, 784).to(device)\n",
    "test_targets = test_targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model, loss function and optimizer\n",
    "### We use softmax regression with cross entropy loss, as it's convex\n",
    "weights = torch.zeros(784, 10, requires_grad=True, device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = NewtonCG([weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0,  loss: 2.30e+00,  test accuracy: 9.80\nepoch: 1,  loss: 5.14e-01,  test accuracy: 85.49\nepoch: 2,  loss: 3.65e-01,  test accuracy: 90.25\nepoch: 3,  loss: 3.15e-01,  test accuracy: 91.45\nepoch: 4,  loss: 2.88e-01,  test accuracy: 92.03\nepoch: 5,  loss: 2.78e-01,  test accuracy: 92.18\nepoch: 6,  loss: 2.73e-01,  test accuracy: 92.31\nepoch: 7,  loss: 2.68e-01,  test accuracy: 92.31\nepoch: 8,  loss: 2.65e-01,  test accuracy: 92.37\nepoch: 9,  loss: 2.62e-01,  test accuracy: 92.38\n"
    }
   ],
   "source": [
    "## Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    outputs = torch.mm(test_inputs, weights)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += test_targets.size(0)\n",
    "    correct += (predicted == test_targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch.mm(train_inputs, weights)\n",
    "    loss = criterion(outputs, train_targets)\n",
    "    loss.backward()\n",
    "    closure = lambda : criterion(torch.mm(train_inputs, weights), train_targets)\n",
    "    loss = optimizer.step(closure)\n",
    "\n",
    "    print(\"epoch: {},  loss: {:.2e},  \"\n",
    "          \"test accuracy: {:.2f}\".format(epoch, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}